'''
supernet for sentence encoder
'''

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple
from .ops import PRIMITIVES, OPS, ConvBN

class MultiheadAttention(nn.Module):

    def __init__(self, embed_dim, num_heads, dropout=0., add_zero_attn=False,
                batch_first=False, device=None, dtype=None) -> None:
        factory_kwargs = {'device': device, 'dtype': dtype}
        super(MultiheadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.kdim = embed_dim
        self.vdim = embed_dim
        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim

        self.num_heads = num_heads
        self.dropout = dropout
        self.batch_first = batch_first
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, "embed_dim must be divisible by num_heads"

        I = torch.eye(embed_dim)
        I3 = torch.cat([torch.eye(embed_dim), torch.eye(embed_dim), torch.eye(embed_dim)], dim=0)

        self.in_proj_weight = nn.Parameter(I3, requires_grad=False)
        self.register_parameter('q_proj_weight', None)
        self.register_parameter('k_proj_weight', None)
        self.register_parameter('v_proj_weight', None)

        self.register_parameter('in_proj_bias', None)
        self.out_proj_weight = nn.Parameter(I, requires_grad=False)
        self.out_proj_bias = nn.Parameter(torch.zeros(embed_dim), requires_grad=False)
        
        self.bias_k = self.bias_v = None

        self.add_zero_attn = add_zero_attn

    def __setstate__(self, state):
        # Support loading old MultiheadAttention checkpoints generated by v1.1.0
        if '_qkv_same_embed_dim' not in state:
            state['_qkv_same_embed_dim'] = True

        super(MultiheadAttention, self).__setstate__(state)

    def forward(self, query, key, value, key_padding_mask = None, attn_mask = None):
        # B D S -> S B D
        query, key, value = [x.permute(2, 0, 1) for x in (query, key, value)]

        attn_output, _ = F.multi_head_attention_forward(
            query, key, value, self.embed_dim, self.num_heads,
            self.in_proj_weight, self.in_proj_bias,
            self.bias_k, self.bias_v, self.add_zero_attn,
            self.dropout, self.out_proj_weight, self.out_proj_bias,
            training=self.training,
            key_padding_mask=key_padding_mask, need_weights=False,
            attn_mask=attn_mask)
        return attn_output.permute(1, 2, 0)

def attention_origin(query, key, value, mask, head=8):
    b, d, s = key.size()
    d_h = d // head
    
    # query, key, value. B S D
    x1 = query.reshape(b * head, d_h, s)
    x2 = key.reshape(b * head, d_h, s)
    x3 = value.reshape(b * head, d_h, s)
    # => B*H D_H S

    attn = x2.permute(0, 2, 1) @ x1 # s_k x s_q
    attn = attn / np.sqrt(d_h)
    attn = attn.masked_fill(~mask[:,None,:,None].repeat(1, head, 1, 1).reshape(b * head, s, 1), -np.inf)
    # attn = F.dropout(attn, p=self.dropout, training=self.training)
    attn = F.softmax(attn, dim=1)

    x = x3 @ attn
    x = x.reshape(b, d, s)
    return x

class AggLayer(nn.Module):
    def __init__(self, dim, head, dropout=0.0, norm='ln', aug_dropouts=[0.0, 0.1]):
        super().__init__()
        self.dim = dim
        self.head = head
        self.dropout = dropout
        self.aug_dropouts = aug_dropouts
        self.norm = norm
        if self.norm == 'ln':
            self.ln = nn.LayerNorm(self.dim)
        elif self.norm == 'bn':
            self.bn = nn.BatchNorm1d(self.dim)
        # self.ln = nn.LayerNorm(self.dim)
        self.attention = MultiheadAttention(dim, head, dropout=self.aug_dropouts[0])
    
    def forward(self, x1, x2, x3, mask, type=0):
        if type == 0:
            x = x1 + x2
            x = F.dropout(x, p=self.dropout, training=self.training)

        else:
            x = self.attention(x1, x2, x3, ~mask)
            '''
            b, d, s = x1.size()
            d_h = d // self.head
            
            # query, key, value. B S D
            x1 = x1.reshape(b * self.head, d_h, s)
            x2 = x2.reshape(b * self.head, d_h, s)
            x3 = x3.reshape(b * self.head, d_h, s)
            # => B*H D_H S

            attn = x2.permute(0, 2, 1) @ x1 # s_k x s_q
            attn = attn / np.sqrt(d_h)
            attn = attn.masked_fill(~mask[:,None,:,None].repeat(1, self.head, 1, 1).reshape(b * self.head, s, 1), -np.inf)
            # attn = F.dropout(attn, p=self.dropout, training=self.training)
            attn = F.softmax(attn, dim=1)

            x = x3 @ attn
            x = x.reshape(b, d, s)
            '''
            x = F.dropout(x, p=self.aug_dropouts[1], training=self.training)

            if self.norm == 'bn':
                x = self.bn(x)
            elif self.norm == 'ln':
                x = self.ln(x.permute(0, 2, 1)).permute(0, 2, 1)

        #x = x.permute(0, 2, 1)
        #x = self.ln(x)
        #x = x.permute(0, 2, 1)
        return x

class Encoder(nn.Module):
    def __init__(self, dim, head, layer, edgeops, nodeops, arch=None, dropout=0.1, context='fc', act='nn.ReLU', norm='ln', pre=True, aug_dropouts=[0.0, 0.1]):
        super().__init__()
        self.arch = arch
        self.edgeops = edgeops
        self.nodeops = nodeops
        self.layer = layer if arch is None else len(arch)
        self.dim = dim
        self.head = head
        self.dropout = dropout
        self.context = context
    
        op_map = {}
        
        def O(idx):
            return OPS[PRIMITIVES[idx]](self.dim, self.dropout, act=eval(act), norm=norm, pre=pre)

        if arch is None:
            # include all edges according to contexts
            for i in range(1, layer + 1):
                for j in range(i):
                    for op in edgeops:
                        for ftype in range(2):
                            for ttype in range(4):
                                op_map[self._get_path_name(j, i, op, ftype, ttype)] = O(op)
                # agg layer
                op_map[f'layer-{i}'] = AggLayer(self.dim, self.head, self.dropout, norm, aug_dropouts=aug_dropouts)
            
        else:
            for i, a in enumerate(arch):
                o1, node, o2, o3, n = a
                cur_id = i + 1
                ftype_prev = None if node <= 0 else arch[node - 1][-1]
                ftype_i = None if i == 0 else arch[i - 1][-1]
                if n == 0:
                    op_map[self._get_path_name(i, cur_id, o1, ftype_i, 0)] = O(o1)
                    if node >= 0:
                        op_map[self._get_path_name(node, cur_id, o2, ftype_prev, 0)] = O(o2)
                else:
                    op_map[self._get_path_name(i, cur_id, o1, ftype_i, 1)] = O(o1)
                    op_map[self._get_path_name(node, cur_id, o2, ftype_prev, 2)] = O(o2)
                    op_map[self._get_path_name(node, cur_id, o3, ftype_prev, 3)] = O(o3)

                # agg layer
                op_map[f'layer-{cur_id}'] = AggLayer(self.dim, self.head, self.dropout, norm, aug_dropouts=aug_dropouts)

        self.op_map = nn.ModuleDict(op_map)

    def _get_path_name(self, fid, tid, op, ftype=None, ttype=None):
        if self.context == 'fc':
            if fid == 0:
                return f'0-{tid}-{op}-{ttype}'
            return f'{fid}-{tid}-{op}-{ftype}-{ttype}'
        elif self.context == 'tc':
            return f'{fid}-{tid}-{op}-{ttype}'
        elif self.context == 'sc':
            if fid == 0:
                return f'0-{tid}-{op}'
            return f'{fid}-{tid}-{op}-{ftype}'
        return f'{fid}-{tid}-{op}'
    
    def get_path_parameters_name(self, arch):
        key_set = set()
        for i, a in enumerate(arch):
            o1, prev, o2, o3, n = a
            f_prev = None if prev <= 0 else arch[prev-1][-1]
            f_i = None if i == 0 else arch[i - 1][-1]
            if n == 0:
                key_set.add(self._get_path_name(i, i + 1, o1, f_i, 0))
                if prev >= 0:
                    key_set.add(self._get_path_name(prev, i + 1, o2, f_prev, 0))
            else:
                key_set.add(self._get_path_name(i, i + 1, o1, f_i, 1))
                key_set.add(self._get_path_name(prev, i + 1, o2, f_prev, 2))
                key_set.add(self._get_path_name(prev, i + 1, o3, f_prev, 3))
        return key_set
    
    def get_parameters_by_name(self, names):
        param_list = []
        for name in names:
            param_list.extend(list(self.op_map[name].parameters()))
        return param_list

    def get_path_parameters(self, arch):
        param_list = []
        key_list = []
        def add_param(path):
            if path not in key_list:
                key_list.append(path)
                param_list.extend(list(self.op_map[path].parameters()))

        for i, a in enumerate(arch):
            o1, prev, o2, o3, n = a
            ftype = None if prev <= 0 else arch[prev - 1][-1]
            ftype_prev = None if prev <= 0 else arch[prev - 1][-1]
            ftype_i = None if i == 0 else arch[i - 1][-1]
            if n == 0:
                add_param(self._get_path_name(i, i + 1, o1, ftype_i, 0))
                if prev >= 0:
                    add_param(self._get_path_name(prev, i + 1, o2, ftype_prev, 0))
            elif n == 1:
                add_param(self._get_path_name(i, i + 1, o1, ftype_i, 1))
                add_param(self._get_path_name(prev, i + 1, o2, ftype_prev, 2))
                add_param(self._get_path_name(prev, i + 1, o3, ftype_prev, 3))

        return param_list

    def forward(self, x, mask, arch=None):
        
        if arch is None or self.arch is not None:
            arch = self.arch

        x_list = [x] + [torch.zeros_like(x) for _ in range(len(arch))]

        for i, a in enumerate(arch):
            cur_idx = i + 1
            o1, prev, o2, o3, n = a
            ftype_prev = None if prev <= 0 else arch[prev - 1][-1]
            ftype_i = None if i == 0 else arch[i - 1][-1]
            if n == 0:
                inp1 = x_list[i]
                # inp1 = F.layer_norm(inp1, self.dim)
                # inp1 = F.dropout(inp1, p=self.dropout, training=self.training)
                feat1 = self.op_map[self._get_path_name(i, i + 1, o1, ftype_i, 0)](inp1, mask=mask)
                if prev >= 0:
                    inp2 = x_list[prev]
                    # inp2 = F.layer_norm(inp2, self.dim)
                    # inp2 = F.dropout(inp2, p=self.dropout, training=self.training)
                    feat2 = self.op_map[self._get_path_name(prev, i + 1, o2, ftype_prev, 0)](inp2, mask=mask)
                else:
                    feat2 = 0
                feat3 = 0
            
            else:
                inp1 = x_list[i]
                # inp1 = F.layer_norm(inp1, self.dim)
                # inp1 = F.dropout(inp1, p=self.dropout, training=self.training)
                inp2 = x_list[prev]
                # inp2 = F.layer_norm(inp2, self.dim)
                # inp2 = F.dropout(inp2, p=self.dropout, training=self.training)
                inp3 = x_list[prev]
                # inp3 = F.layer_norm(inp3, self.dim)
                # inp3 = F.dropout(inp3, p=self.dropout, training=self.training)
                feat1 = self.op_map[self._get_path_name(i, i + 1, o1, ftype_i, 1)](inp1, mask=mask)
                feat2 = self.op_map[self._get_path_name(prev, i + 1, o2, ftype_prev, 2)](inp2, mask=mask)
                feat3 = self.op_map[self._get_path_name(prev, i + 1, o3, ftype_prev, 3)](inp3, mask=mask)
            
            x = self.op_map[f'layer-{cur_idx}'](feat1, feat2, feat3, mask, n)

            x_list[cur_idx] = x

        out = x_list[-1]
        # out = F.layer_norm(out, self.dim)
        # out = F.dropout(out, p=self.dropout, training=self.training)
        return out

class TextClassifier(nn.Module):
    def __init__(self, embeddings, dim, head, nclass, layer=None, edgeops=None, nodeops=None, arch=None, dropout=0.0, context='fc', act='nn.ReLU', norm='ln', pre=True, freeze=True, pad_idx=0, aug_dropouts=[0.0, 0.1]) -> None:
        super().__init__()
        self.emb = nn.Embedding.from_pretrained(embeddings, padding_idx=pad_idx, freeze=freeze)
        # self.stem = nn.Linear(embeddings.size(1), dim)
        self.stem = ConvBN(1, embeddings.size(1), dim, 1 - dropout, False, True, False, act=='nn.ReLU', norm=='ln')
        self.core = Encoder(dim, head, layer, edgeops, nodeops, arch=arch, dropout=dropout, context=context, act=act, norm=norm, pre=pre, aug_dropouts=aug_dropouts)
        self.classifier = nn.Linear(dim, nclass)
        nn.init.normal_(self.classifier.weight, 0, 0.02)
        nn.init.constant_(self.classifier.bias, 0)
        self.dropout = dropout

    def forward(self, x, mask, arch=None, sliding=False, window_size=64, stride=32):
        if not sliding:
            x = self.emb(x)
            x = x.permute(0, 2, 1)
            # x = F.dropout(x, p=self.dropout, training=self.training)
            x = self.stem(x, mask=mask)
            # x = F.dropout(x, p=self.dropout, training=self.training)
            x = self.core(x, mask, arch)
            # x = F.dropout(x, p=self.dropout, training=self.training)
            # max-pooling
            x = x.masked_fill(~mask[:,None,:], -np.inf)
            x = x.max(dim=2)[0]
            x = self.classifier(x)
        else:
            x = self.emb(x)
            x = x.permute(0, 2, 1)
            b, d, s = x.size()
            maxs = []
            begin_idx = 0
            while begin_idx < s:
                x_sub = x[:,:,begin_idx:begin_idx + window_size]
                masks = mask[:, begin_idx: begin_idx + window_size]
                lengths = masks.sum(dim=1)
                length_mask = lengths > 0
                x_sub = x_sub[length_mask]
                x_sub = self.stem(x_sub, mask=masks[length_mask])
                x_sub = self.core(x_sub, masks[length_mask], arch)
                x_sub = x_sub.masked_fill(~masks[length_mask,None,:], -np.inf)
                x_sub = x_sub.max(dim=2)[0]
                x_ful = torch.zeros(b, x_sub.size(1)).to(x_sub)
                x_ful[length_mask] = x_sub
                x_ful[~length_mask] = -np.inf
                maxs.append(x_ful[:,:,None])
                begin_idx += stride
            x = torch.cat(maxs, dim=2).max(dim=2)[0]
            x = self.classifier(x)
        return x
        
    def get_path_parameters(self, arch):
        return self.core.get_path_parameters(arch)
    
    def get_path_parameters_name(self, arch):
        return self.core.get_path_parameters_name(arch)
    
    def get_parameters_by_name(self, names):
        return self.core.get_parameters_by_name(names)
    
    @classmethod
    def add_args(cls, parser):
        parser.add_argument('--dim', type=int, default=64, help='dimension of model')
        parser.add_argument('--head', type=int, default=4, help='attn head number of model')
        parser.add_argument('--dropout', type=float, default=0.7, help='dropout of model')
        parser.add_argument('--context', type=str, default='fc', choices=['fc', 'sc', 'tc', 'nc'], help='context constraint')
        return parser

    @classmethod
    def build_from_args(cls, embeddings, nclass, args, edgeops=None, nodeops=None, layer=None, arch=None):
        return cls(embeddings, args.dim, args.head, nclass, layer, edgeops, nodeops, arch, args.dropout, args.context)
